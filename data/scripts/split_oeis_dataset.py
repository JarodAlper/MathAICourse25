#!/usr/bin/env python3
#Prompt for Claude 4.5 Sonnet inside Cursor: Using the scikit-learn library, can you write a simple script that splits a dataset such as oeis-dataset into 75% training data and 25% test data
"""
OEIS Dataset Splitter using scikit-learn

This script splits the OEIS dataset into training (75%) and test (25%) sets
using scikit-learn's train_test_split function.
"""

from sklearn.model_selection import train_test_split
import argparse
import os
from typing import Tuple, List


def parse_oeis_dataset(file_path: str) -> Tuple[List[str], List[List[int]]]:
    """
    Parse the OEIS dataset file and extract sequence IDs and values.
    
    Args:
        file_path: Path to the OEIS dataset file
        
    Returns:
        Tuple of (sequence_ids, sequence_values)
    """
    sequence_ids = []
    sequence_values = []
    
    print(f"Reading dataset from {file_path}...")
    
    with open(file_path, 'r') as file:
        for line_num, line in enumerate(file, 1):
            line = line.strip()
            
            # Skip empty lines and comments
            if not line or line.startswith('#'):
                continue
                
            # Split by comma to separate ID from values
            parts = line.split(',')
            if len(parts) < 2:
                print(f"Warning: Skipping malformed line {line_num}: {line[:50]}...")
                continue
                
            sequence_id = parts[0].strip()
            try:
                # Convert values to integers, handling any parsing errors
                values = [int(x.strip()) for x in parts[1:] if x.strip()]
                if values:  # Only add if we have valid values
                    sequence_ids.append(sequence_id)
                    sequence_values.append(values)
            except ValueError as e:
                print(f"Warning: Skipping line {line_num} due to parsing error: {e}")
                continue
                
            # Progress indicator for large files
            if line_num % 10000 == 0:
                print(f"Processed {line_num} lines...")
    
    print(f"Successfully parsed {len(sequence_ids)} sequences")
    return sequence_ids, sequence_values


def split_dataset(sequence_ids: List[str], 
                 sequence_values: List[List[int]], 
                 test_size: float = 0.25,
                 random_state: int = 42) -> Tuple[List[str], List[str], List[List[int]], List[List[int]]]:
    """
    Split the dataset into training and test sets using scikit-learn.
    
    Args:
        sequence_ids: List of sequence identifiers
        sequence_values: List of sequence value lists
        test_size: Proportion of dataset to use for testing (default: 0.25)
        random_state: Random seed for reproducibility (default: 42)
        
    Returns:
        Tuple of (train_ids, test_ids, train_values, test_values)
    """
    print(f"Splitting dataset with test_size={test_size} and random_state={random_state}")
    
    # Use scikit-learn's train_test_split
    train_ids, test_ids, train_values, test_values = train_test_split(
        sequence_ids,
        sequence_values,
        test_size=test_size,
        random_state=random_state,
        shuffle=True
    )
    
    print(f"Training set: {len(train_ids)} sequences")
    print(f"Test set: {len(test_ids)} sequences")
    
    return train_ids, test_ids, train_values, test_values


def save_split_datasets(train_ids: List[str], 
                       test_ids: List[str],
                       train_values: List[List[int]], 
                       test_values: List[List[int]],
                       output_dir: str = "."):
    """
    Save the split datasets to separate files.
    
    Args:
        train_ids: Training set sequence IDs
        test_ids: Test set sequence IDs
        train_values: Training set sequence values
        test_values: Test set sequence values
        output_dir: Directory to save output files
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # Save training set
    train_file = os.path.join(output_dir, "oeis_train.txt")
    with open(train_file, 'w') as f:
        f.write("# OEIS Training Dataset (75%)\n")
        f.write("# Generated by split-dataset.py\n\n")
        for seq_id, values in zip(train_ids, train_values):
            values_str = ','.join(map(str, values))
            f.write(f"{seq_id},{values_str}\n")
    
    # Save test set
    test_file = os.path.join(output_dir, "oeis_test.txt")
    with open(test_file, 'w') as f:
        f.write("# OEIS Test Dataset (25%)\n")
        f.write("# Generated by split-dataset.py\n\n")
        for seq_id, values in zip(test_ids, test_values):
            values_str = ','.join(map(str, values))
            f.write(f"{seq_id},{values_str}\n")
    
    print(f"Training set saved to: {train_file}")
    print(f"Test set saved to: {test_file}")


def main():
    """Main function to orchestrate the dataset splitting process."""
    parser = argparse.ArgumentParser(description="Split OEIS dataset into training and test sets")
    parser.add_argument("--input", "-i", 
                       default="oeis-dataset",
                       help="Input dataset file path (default: oeis-dataset)")
    parser.add_argument("--test-size", "-t", 
                       type=float, 
                       default=0.25,
                       help="Proportion of data for testing (default: 0.25)")
    parser.add_argument("--random-state", "-r", 
                       type=int, 
                       default=42,
                       help="Random seed for reproducibility (default: 42)")
    parser.add_argument("--output-dir", "-o", 
                       default=".",
                       help="Output directory for split datasets (default: current directory)")
    
    args = parser.parse_args()
    
    # Validate input file exists
    if not os.path.exists(args.input):
        print(f"Error: Input file '{args.input}' not found!")
        return 1
    
    # Validate test_size
    if not 0 < args.test_size < 1:
        print(f"Error: test_size must be between 0 and 1, got {args.test_size}")
        return 1
    
    try:
        # Parse the dataset
        sequence_ids, sequence_values = parse_oeis_dataset(args.input)
        
        if len(sequence_ids) == 0:
            print("Error: No valid sequences found in the dataset!")
            return 1
        
        # Split the dataset
        train_ids, test_ids, train_values, test_values = split_dataset(
            sequence_ids, 
            sequence_values, 
            test_size=args.test_size,
            random_state=args.random_state
        )
        
        # Save the split datasets
        save_split_datasets(train_ids, test_ids, train_values, test_values, args.output_dir)
        
        print("\nDataset splitting completed successfully!")
        print(f"Total sequences: {len(sequence_ids)}")
        print(f"Training sequences: {len(train_ids)} ({len(train_ids)/len(sequence_ids)*100:.1f}%)")
        print(f"Test sequences: {len(test_ids)} ({len(test_ids)/len(sequence_ids)*100:.1f}%)")
        
        return 0
        
    except Exception as e:
        print(f"Error: {e}")
        return 1


if __name__ == "__main__":
    exit(main())
